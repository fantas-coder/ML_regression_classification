# Лабораторная работа по линейной регрессии

## Описание
Эта лабораторная работа посвящена анализу данных из файла `data_v1-06.csv` с использованием методов линейной регрессии: обычного МНК (OLS) и взвешенного МНК (WLS). Цель — исследовать зависимость целевой переменной `'y'` от признака `'x'`, обработать гетероскедастичность, проверить нормальность остатков и сравнить эффективность моделей. Работа выполнена в Python с использованием библиотек `pandas`, `numpy`, `statsmodels`, `matplotlib`, `seaborn` и `scipy`.

## Данные
- **Файл**: `data_v1-06.csv` (80 строк, 2 столбца: `'x'` — признак, `'y'` — целевая).
- **Предобработка**: Удалён один выброс (`'y' = 12.134`), итог — 79 строк. Разделение на обучающую (70%, 56 строк) и тестовую (30%, 23 строки) выборки с `random_state=42`.
- **Характеристики**:
  - Корреляция `'x'` и `'y'`: 0.8421 (сильная линейная зависимость).
  - `'y'`: Логнормальное распределение (Omnibus p=0.004, Skew=0.731).
  - Гетероскедастичность: Дисперсия остатков растёт с `'x'` (0.185598–4.362228).

## Задания и результаты
1. **OLS (простая линейная регрессия)**:
   - Модель: \( y = -0.0710 + 3.6799 \cdot x \).
   - R²: 0.713 (train), 0.673 (test).
   - Остатки: Ненормальны на train (\( \chi^2 \) p=0.00188, Шапиро p=0.01068), нормальны на test (p>0.05).
   - ДИ: Широкие из-за гетероскедастичности.

2. **Анализ гетероскедастичности**:
   - Графики остатков: "Воронка" (разброс растёт с `'x'`).
   - Группы по `'x'`: Дисперсии 0.260768–4.362228, рост с `'x'`.

3. **Регрессия модулей остатков**:
   - Модель: \( |e_i| = \gamma_0 + \gamma_1 \cdot x_i \).
   - Результат: F=16.826, p≈0.0, гетероскедастичность подтверждена.

4. **WLS (взвешенная линейная регрессия)**:
   - **WLS а**: Веса \( w_i = 1 / (0.001 + f_i(x_i)) \), R²: 0.799 (train), 0.750 (test).
   - **WLS б**: Веса \( w_i = 1 / (0.001 + x_i) \), R²: 0.895 (train), 0.929 (test).
   - ДИ: WLS б — самые узкие, WLS а — уже, чем OLS.
   - Графики: WLS б лучше описывает данные.

5. **Проверка нормальности остатков**:
   - **OLS**: Ненормальны на train из-за логнормальности `'y'`.
   - **WLS а**: Нормальны (\( \chi^2 \) p=0.29093, Шапиро p=0.69711 train).
   - **WLS б**: Ненормальны из-за выброса (~25 в test, \( \chi^2 \) p=0.0, Шапиро p=0.0).
   - Тесты: \( \chi^2 \), Шапиро-Уилк; гистограммы с KDE.

6. **Сравнение OLS и WLS**:
   - OLS: Худшая точность (R²_test=0.673), ненормальные остатки, широкие ДИ.
   - WLS: Корректирует гетероскедастичность, улучшает R² и ДИ.
   - WLS б: Лучшая по R²_test=0.929, но ненормальные остатки из-за выброса.
   - WLS а: Баланс между точностью и нормальностью.

## Вывод
WLS б — лучшая модель по R² (0.929 test), эффективно корректирует гетероскедастичность, но выброс в остатках нарушает нормальность. WLS а — компромисс (R²_test=0.750, нормальные остатки). OLS наименее точна из-за гетероскедастичности. Для улучшения нормальности рекомендуется лог-трансформация `'y'` или робастные методы (cov_type='HC3').

# Лабораторная работа: ECOC-классификация (Error-Correcting Output Codes)

**Цель:** Исследовать влияние способов кодирования и декодирования ECOC на точность многоклассовой классификации с использованием байесовских бинарных классификаторов (QDA).

### Этапы работы

1. **EDA и предобработка**  
   Загружен и очищен датасет (500 → 497 объекта, 2 признака).  
   Классы сбалансированы, распределения близки к нормальным, но наблюдается пересечение.

2. **Реализация базового классификатора**  
   Собственный `BayesianBinaryClassifier` — квадратичный дискриминантный анализ (QDA) с равными априорными вероятностями (по условию задачи).

3. **ECOC-кодирование (4 стратегии)**  
   - OVA (One-vs-All) — 4 бита  
   - OVO (One-vs-One) — 6 бит, тернарный  
   - Полное бинарное (exhaustive code) — 7 бит, расстояние Хэмминга = 4  
   - Полное тернарное (бинарный код Хэмминга 7,4) — 7 бит, расстояние = 3

4. **ECOC-декодирование (2 способа)**  
   - Невзвешенное: по расстоянию Хэмминга / евклидову  
   - Взвешенное: линейное loss-based, с очками `f_k = 2·p_k − 1`

   → Всего 8 многоклассовых классификаторов.

5. **Оценка качества**  
   - Accuracy на train/test  
   - Micro- и macro-averaged ROC/PR-кривые  
   - ROC AUC и PR AUC

6. **Визуализация**  
   - Области классов и границы решений (8 + общий график)  
   - Сравнение точности (bar chart с таблицей)

### Ключевые результаты

- Лучшая модель: **OVA + взвешенное декодирование**  
  → Accuracy = 0.707, ROC AUC micro = 0.922, macro = 0.904
- Избыточные коды длины 7 (бинарный и Хэмминг-подобный) **не дали преимущества** на сложных данных.
- Взвешенное декодирование помогло **только OVA** (+4 п.п.).
- Macro-анализ показал: **OVA — наиболее сбалансированная** по классам.

### Главный вывод

> На реальных данных с пересекающимися классами **простая и устойчивая комбинация OVA + взвешенное декодирование** превосходит сложные избыточные коды, которые требуют идеально откалиброванных базовых классификаторов.
